{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsdIl9xx27MWpa3wvMohFd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mperetto/Python-Chat-with-audio/blob/main/Chat_with_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with audio interviews\n",
        "This ongoing project aims to develop a chat app that allows users to upload an audio file and view the chat history of the file in the format of 'speaker_1: ... speaker_2: ...,' with the ability to ask questions about the interview.\n",
        "\n",
        "## Project Structure\n",
        "The project is subdivided in 5 main parts.\n",
        "\n",
        "1.   Upload and conversion to .wav format of the audio file (using ffmpeg) in case it is not already in .wav format.\n",
        "2.   Processing the diarization of the audio file using the pyannote model to recognize the speakers in the audio.\n",
        "3.   Extract the text from the audio with Whisper model.\n",
        "4.   Matching the audio text segment get by Whisper with the timing of the diarization.\n",
        "5.   Visualize the extracted text in chat format and permit the Q&A.\n",
        "\n"
      ],
      "metadata": {
        "id": "lkygWDiIKQOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the dependecies"
      ],
      "metadata": {
        "id": "KR0MqoeOTau4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlaMtQsCKBBX"
      },
      "outputs": [],
      "source": [
        "!apt update && apt install ffmpeg\n",
        "!pip3 install -q git+https://github.com/linto-ai/whisper-timestamped\n",
        "!pip install -q https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import and set up the pyannote pipeline"
      ],
      "metadata": {
        "id": "Qv5RKC8NTiaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "import torch\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    'pyannote/speaker-diarization@2.1',\n",
        "    use_auth_token='YOUR_TOKEN' # To get the token execute this cell and follow the instructions\n",
        ")\n",
        "pipeline.to(torch.device('cuda'))"
      ],
      "metadata": {
        "id": "iwWbD24qTTdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import audio file and convert it if necessary"
      ],
      "metadata": {
        "id": "gePp_kyLTre_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "path = \"audio.m4a\"\n",
        "\n",
        "if path[-3:] != 'wav':\n",
        "  subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
        "  path = 'audio.wav'"
      ],
      "metadata": {
        "id": "n0guN6ZjT12w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the diarization"
      ],
      "metadata": {
        "id": "OPghPVTwUXbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diarization = pipeline(path, num_speakers=2)"
      ],
      "metadata": {
        "id": "NC53b13TUNxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    print(f\"[{turn.start:.1f}, {turn.end:.1f}, {speaker}]\")"
      ],
      "metadata": {
        "id": "qwsRhiNiWfl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Whisper model and extract the text"
      ],
      "metadata": {
        "id": "czUev3uxU3sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper_timestamped as whisper\n",
        "\n",
        "audio = whisper.load_audio(path)\n",
        "\n",
        "model = whisper.load_model('medium')\n",
        "\n",
        "result = whisper.transcribe(model, audio, vad=True)"
      ],
      "metadata": {
        "id": "bf0_xJxKVJmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"text\"]"
      ],
      "metadata": {
        "id": "OCGTfYRQWm8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_timestamp = []\n",
        "for segment in result[\"segments\"]:\n",
        "  words_timestamp.extend(segment[\"words\"])"
      ],
      "metadata": {
        "id": "NOHkyFOEWpLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_timestamp"
      ],
      "metadata": {
        "id": "RR_rQQhT5Q-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(diarization)"
      ],
      "metadata": {
        "id": "6ZJP4PA-6Lmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prev_speaker = \"\"\n",
        "chat_history = []\n",
        "map_speakers = {\"SPEAKER_00\": \"speaker_0\", \"SPEAKER_01\": \"speaker_1\"}\n",
        "cont = 0\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    cont += 1\n",
        "    if prev_speaker == \"\":\n",
        "      chat_history.append({\"role\": map_speakers[speaker], \"message\": \"\"})\n",
        "      prev_speaker = speaker\n",
        "    elif prev_speaker != speaker:\n",
        "      chat_history.append({\"role\": map_speakers[speaker], \"message\": \"\"})\n",
        "      prev_speaker = speaker\n",
        "\n",
        "    for word in words_timestamp:\n",
        "      if word[\"start\"] >= round(turn.start, 1) and word[\"end\"] <= round(turn.end):\n",
        "        chat_history[-1][\"message\"] += f\" {word['text']}\""
      ],
      "metadata": {
        "id": "DYqfHMsyyczb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont"
      ],
      "metadata": {
        "id": "-cuSJbnJIsqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "id": "bsByM77s-uJF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}